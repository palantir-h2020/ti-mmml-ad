{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML model pipelines on Distributed Deep Neural Nets\n",
    "\n",
    "This notebook describes how to build machine learning [pipelines with Spark ML](http://spark.apache.org/docs/latest/ml-guide.html) for distributed versions of Keras deep learning models. As data set we use the Otto Product Classification challenge from Kaggle. The reason we chose this data is that it is small and very structured. This way, we can focus more on technical components rather than prepcrocessing intricacies. Also, users with slow hardware or without a full-blown Spark cluster should be able to run this example locally, and still learn a lot about the distributed mode.\n",
    "\n",
    "Often, the need to distribute computation is not imposed by model training, but rather by building the data pipeline, i.e. ingestion, transformation etc. In training, deep neural networks tend to do fairly well on one or more GPUs on one machine. Most of the time, using gradient descent methods, you will process one batch after another anyway. Even so, it may still be beneficial to use frameworks like Spark to integrate your models with your surrounding infrastructure. On top of that, the convenience provided by Spark ML pipelines can be very valuable (being syntactically very close to what you might know from [```scikit-learn```](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)).\n",
    "\n",
    "**TL;DR:** We will show how to tackle a classification problem using distributed deep neural nets and Spark ML pipelines in an example that is essentially a distributed version of the one found [here](https://github.com/fchollet/keras/blob/master/examples/kaggle_otto_nn.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using this notebook\n",
    "As we are going to use elephas, you will need access to a running Spark context to run this notebook. If you don't have it already, install Spark locally by following the [instructions provided here](https://github.com/maxpumperla/elephas/blob/master/README.md). Make sure to also export ```SPARK_HOME``` to your path and start your ipython/jupyter notebook as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "IPYTHON_OPTS=\"notebook\" ${SPARK_HOME}/bin/pyspark --driver-memory 4G elephas/examples/Spark_ML_Pipeline.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your environment, try to print the Spark context (provided as ```sc```), i.e. execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x1132d61d0>\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otto Product Classification Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and test data is available [here](https://www.kaggle.com/c/otto-group-product-classification-challenge/data). Go ahead and download the data. Inspecting it, you will see that the provided csv files consist of an id column, 93 integer feature columns. ```train.csv``` has an additional column for labels, which ```test.csv``` is missing. The challenge is to accurately predict test labels. For the rest of this notebook, we will assume data is stored at ```data_path```, which you should modify below as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"./\" # <-- Make sure to adapt this to where your csv files are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data is relatively simple, but we have to take care of a few things. First, while you can shuffle rows of an  RDD, it is generally not very efficient. But since data in ```train.csv``` is sorted by category, we'll have to shuffle in order to make the model perform well. This is what the function ```shuffle_csv``` below is for. Next, we read in plain text in ```load_data_rdd```, split lines by comma and convert features to float vector type. Also, note that the last column in ```train.csv``` represents the category, which has a ```Class_``` prefix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Data Frames\n",
    "\n",
    "Spark has a few core data structures, among them is the ```data frame```, which is a distributed version of the named columnar data structure many will now from either [R](https://stat.ethz.ch/R-manual/R-devel/library/base/html/data.frame.html) or [Pandas](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html). We need a so called ```SQLContext``` and an optional column-to-names mapping to create a data frame from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "def shuffle_csv(csv_file):\n",
    "    lines = open(csv_file).readlines()\n",
    "    random.shuffle(lines)\n",
    "    open(csv_file, 'w').writelines(lines)\n",
    "\n",
    "def load_data_frame(csv_file, shuffle=True, train=True):\n",
    "    if shuffle:\n",
    "        shuffle_csv(csv_file)\n",
    "    data = sc.textFile(data_path + csv_file) # This is an RDD, which will later be transformed to a data frame\n",
    "    data = data.filter(lambda x:x.split(',')[0] != 'id').map(lambda line: line.split(','))\n",
    "    if train:\n",
    "        data = data.map(\n",
    "            lambda line: (Vectors.dense(np.asarray(line[1:-1]).astype(np.float32)),\n",
    "                          str(line[-1])) )\n",
    "    else:\n",
    "        # Test data gets dummy labels. We need the same structure as in Train data\n",
    "        data = data.map( lambda line: (Vectors.dense(np.asarray(line[1:]).astype(np.float32)),\"Class_1\") ) \n",
    "    return sqlContext.createDataFrame(data, ['features', 'category'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load both train and test data and print a few rows of data using the convenient ```show``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data frame:\n",
      "+--------------------+--------+\n",
      "|            features|category|\n",
      "+--------------------+--------+\n",
      "|[0.0,0.0,0.0,0.0,...| Class_8|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_8|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|\n",
      "|[0.0,1.0,0.0,1.0,...| Class_6|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_9|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_3|\n",
      "|[0.0,0.0,4.0,0.0,...| Class_8|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_7|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Test data frame (note the dummy category):\n",
      "+--------------------+--------+\n",
      "|            features|category|\n",
      "+--------------------+--------+\n",
      "|[1.0,0.0,0.0,1.0,...| Class_1|\n",
      "|[0.0,1.0,13.0,1.0...| Class_1|\n",
      "|[0.0,0.0,1.0,1.0,...| Class_1|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_1|\n",
      "|[2.0,0.0,5.0,1.0,...| Class_1|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_1|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_1|\n",
      "|[0.0,0.0,0.0,1.0,...| Class_1|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_1|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_1|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data_frame(\"train.csv\")\n",
    "test_df = load_data_frame(\"test.csv\", shuffle=False, train=False) # No need to shuffle test data\n",
    "\n",
    "print(\"Train data frame:\")\n",
    "train_df.show(10)\n",
    "\n",
    "print(\"Test data frame (note the dummy category):\")\n",
    "test_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Defining Transformers\n",
    "\n",
    "Up until now, we basically just read in raw data. Luckily, ```Spark ML``` has quite a few preprocessing features available, so the only thing we will ever have to do is define transformations of data frames.\n",
    "\n",
    "To proceed, we will first transform category strings to double values. This is done by a so called ```StringIndexer```. Note that we carry out the actual transformation here already, but that is just for demonstration purposes. All we really need is too define ```string_indexer``` to put it into a pipeline later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "string_indexer = StringIndexer(inputCol=\"category\", outputCol=\"index_category\")\n",
    "fitted_indexer = string_indexer.fit(train_df)\n",
    "indexed_df = fitted_indexer.transform(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it's good practice to normalize the features, which is done with a ```StandardScaler```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=True)\n",
    "fitted_scaler = scaler.fit(indexed_df)\n",
    "scaled_df = fitted_scaler.transform(indexed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of indexing and scaling. Each transformation adds new columns to the data frame:\n",
      "+--------------------+--------+--------------+--------------------+\n",
      "|            features|category|index_category|     scaled_features|\n",
      "+--------------------+--------+--------------+--------------------+\n",
      "|[0.0,0.0,0.0,0.0,...| Class_8|           2.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_8|           2.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|           0.0|[-0.2535060296260...|\n",
      "|[0.0,1.0,0.0,1.0,...| Class_6|           1.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_9|           4.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|           0.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_2|           0.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_3|           3.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,4.0,0.0,...| Class_8|           2.0|[-0.2535060296260...|\n",
      "|[0.0,0.0,0.0,0.0,...| Class_7|           5.0|[-0.2535060296260...|\n",
      "+--------------------+--------+--------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The result of indexing and scaling. Each transformation adds new columns to the data frame:\")\n",
    "scaled_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Deep Learning model\n",
    "\n",
    "Now that we have a data frame with processed features and labels, let's define a deep neural net that we can use to address the classification problem. Chances are you came here because you know a thing or two about deep learning. If so, the model below will look very straightforward to you. We build a keras model by choosing a set of three consecutive Dense layers with dropout and ReLU activations. There are certainly much better architectures for the problem out there, but we really just want to demonstrate the general flow here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.utils import to_categorical, generic_utils\n",
    "\n",
    "nb_classes = train_df.select(\"category\").distinct().count()\n",
    "input_dim = len(train_df.select(\"features\").first()[0])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(input_dim,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Elephas model\n",
    "\n",
    "To lift the above Keras ```model``` to Spark, we define an ```Estimator``` on top of it. An ```Estimator``` is Spark's incarnation of a model that still has to be trained. It essentially only comes with only a single (required) method, namely ```fit```. Once we call ```fit``` on a data frame, we get back a ```Model```, which is a trained model with a ```transform``` method to predict labels.\n",
    "\n",
    "We do this by initializing an ```ElephasEstimator``` and setting a few properties. As by now our input data frame will have many columns, we have to tell the model where to find features and labels by column name. Then we provide serialized versions of our Keras model. We can not plug in keras models into the ```Estimator``` directly, as Spark will have to serialize them anyway for communication with workers, so it's better to provide the serialization ourselves. In fact, while pyspark knows how to serialize ```model```, it is extremely inefficient and can break if models become too large. Spark ML is especially picky (and rightly so) about parameters and more or less prohibits you from providing non-atomic types and arrays of the latter. Most of the remaining parameters are optional and rather self explainatory. Plus, many of them you know if you have ever run a keras model before. We just include them here to show the full set of training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElephasEstimator_415398ab22cb1699f794"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elephas.ml_model import ElephasEstimator\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(adam)\n",
    "\n",
    "# Initialize SparkML Estimator and set all relevant properties\n",
    "estimator = ElephasEstimator()\n",
    "estimator.setFeaturesCol(\"scaled_features\")             # These two come directly from pyspark,\n",
    "estimator.setLabelCol(\"index_category\")                 # hence the camel case. Sorry :)\n",
    "estimator.set_keras_model_config(model.to_yaml())       # Provide serialized Keras model\n",
    "estimator.set_categorical_labels(True)\n",
    "estimator.set_nb_classes(nb_classes)\n",
    "estimator.set_num_workers(1)  # We just use one worker here. Feel free to adapt it.\n",
    "estimator.set_epochs(20) \n",
    "estimator.set_batch_size(128)\n",
    "estimator.set_verbosity(1)\n",
    "estimator.set_validation_split(0.15)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode(\"synchronous\")\n",
    "estimator.set_loss(\"categorical_crossentropy\")\n",
    "estimator.set_metrics(['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkML Pipelines\n",
    "\n",
    "Now for the easy part: Defining pipelines is really as easy as listing pipeline stages. We can provide any configuration of ```Transformers``` and ```Estimators``` really, but here we simply take the three components defined earlier. Note that ```string_indexer``` and ```scaler``` and interchangable, while ```estimator``` somewhat obviously has to come last in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[string_indexer, scaler, estimator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting and evaluating the pipeline\n",
    "\n",
    "The last step now is to fit the pipeline on training data and evaluate it. We evaluate, i.e. transform, on _training data_, since only in that case do we have labels to check accuracy of the model. If you like, you could transform the ```test_df``` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61878/61878 [==============================] - 0s     \n",
      "+--------------+----------+\n",
      "|index_category|prediction|\n",
      "+--------------+----------+\n",
      "|           2.0|       2.0|\n",
      "|           2.0|       2.0|\n",
      "|           0.0|       0.0|\n",
      "|           1.0|       1.0|\n",
      "|           4.0|       4.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           3.0|       3.0|\n",
      "|           2.0|       2.0|\n",
      "|           5.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           4.0|       4.0|\n",
      "|           0.0|       0.0|\n",
      "|           4.0|       1.0|\n",
      "|           2.0|       2.0|\n",
      "|           1.0|       1.0|\n",
      "|           0.0|       0.0|\n",
      "|           6.0|       0.0|\n",
      "|           2.0|       2.0|\n",
      "|           1.0|       1.0|\n",
      "|           2.0|       2.0|\n",
      "|           8.0|       8.0|\n",
      "|           1.0|       1.0|\n",
      "|           5.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       3.0|\n",
      "|           0.0|       0.0|\n",
      "|           1.0|       1.0|\n",
      "|           4.0|       4.0|\n",
      "|           2.0|       2.0|\n",
      "|           0.0|       3.0|\n",
      "|           3.0|       3.0|\n",
      "|           0.0|       0.0|\n",
      "|           3.0|       0.0|\n",
      "|           1.0|       5.0|\n",
      "|           3.0|       3.0|\n",
      "|           2.0|       2.0|\n",
      "|           1.0|       1.0|\n",
      "|           0.0|       0.0|\n",
      "|           2.0|       2.0|\n",
      "|           2.0|       2.0|\n",
      "|           1.0|       1.0|\n",
      "|           6.0|       6.0|\n",
      "|           1.0|       1.0|\n",
      "|           0.0|       3.0|\n",
      "|           7.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           1.0|       1.0|\n",
      "|           1.0|       1.0|\n",
      "|           6.0|       6.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       3.0|\n",
      "|           2.0|       2.0|\n",
      "|           0.0|       0.0|\n",
      "|           2.0|       2.0|\n",
      "|           0.0|       0.0|\n",
      "|           4.0|       4.0|\n",
      "|           0.0|       0.0|\n",
      "|           6.0|       6.0|\n",
      "|           2.0|       5.0|\n",
      "|           0.0|       3.0|\n",
      "|           3.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           3.0|       3.0|\n",
      "|           4.0|       4.0|\n",
      "|           0.0|       3.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           4.0|       4.0|\n",
      "|           3.0|       0.0|\n",
      "|           2.0|       2.0|\n",
      "|           1.0|       1.0|\n",
      "|           7.0|       7.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       3.0|\n",
      "|           1.0|       1.0|\n",
      "|           1.0|       1.0|\n",
      "|           5.0|       4.0|\n",
      "|           1.0|       1.0|\n",
      "|           1.0|       1.0|\n",
      "|           4.0|       4.0|\n",
      "|           3.0|       3.0|\n",
      "|           0.0|       0.0|\n",
      "|           2.0|       2.0|\n",
      "|           4.0|       4.0|\n",
      "|           7.0|       7.0|\n",
      "|           2.0|       2.0|\n",
      "|           0.0|       0.0|\n",
      "|           1.0|       1.0|\n",
      "|           0.0|       0.0|\n",
      "|           4.0|       4.0|\n",
      "|           1.0|       1.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       0.0|\n",
      "|           0.0|       3.0|\n",
      "|           0.0|       3.0|\n",
      "|           0.0|       0.0|\n",
      "+--------------+----------+\n",
      "only showing top 100 rows\n",
      "\n",
      "0.764132648114\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "fitted_pipeline = pipeline.fit(train_df) # Fit model to data\n",
    "\n",
    "prediction = fitted_pipeline.transform(train_df) # Evaluate on train data.\n",
    "# prediction = fitted_pipeline.transform(test_df) # <-- The same code evaluates test data.\n",
    "pnl = prediction.select(\"index_category\", \"prediction\")\n",
    "pnl.show(100)\n",
    "\n",
    "prediction_and_label = pnl.map(lambda row: (row.index_category, row.prediction))\n",
    "metrics = MulticlassMetrics(prediction_and_label)\n",
    "print(metrics.precision())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "It may certainly take some time to master the principles and syntax of both Keras and Spark, depending where you come from, of course. However, we also hope you come to the conclusion that once you get beyond the stage of struggeling with defining your models and preprocessing your data, the business of building and using SparkML pipelines is quite an elegant and useful one. \n",
    "\n",
    "If you like what you see, consider helping further improve elephas or contributing to Keras or Spark. Do you have any constructive remarks on this notebook? Is there something you want me to clarify? In any case, feel free to contact me."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
